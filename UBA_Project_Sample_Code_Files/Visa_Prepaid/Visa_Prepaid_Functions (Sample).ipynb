{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3095401",
   "metadata": {},
   "source": [
    "### Importing the Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DateType, TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6375070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "from pdfminer.high_level import extract_text as fallback_text_extraction\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "current_day = datetime.today().date()\n",
    "#current_day = datetime(2023, 12, 21)#: For debugging files for different dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41848b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9831659c",
   "metadata": {},
   "source": [
    "### Getting the Folder Path from the FolderPath widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd96a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = dbutils.widgets.get(\"FolderPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a622e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa789303",
   "metadata": {},
   "source": [
    "#### A function that tells us when other functions are called and when the functions are done running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_name_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(\"\\n\")\n",
    "        print(f\"FUNCTION CALLED: {func.__name__}\")\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"END OF FUNCTION CALL: {func.__name__}\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e340c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b2bb75f",
   "metadata": {},
   "source": [
    "#### A function for converting the filepath gotten from the widget to one that can be read using pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4703f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def _convert_dbfs_file_path(file_path):\n",
    "    print(\"Converting dbfs file path to a readable file path\")\n",
    "    if file_path.startswith(\"dbfs:\"):\n",
    "        proper_file_path = file_path.replace(\"dbfs:\", \"/dbfs\")\n",
    "        print(f\"Filepath created: {proper_file_path}\")\n",
    "        return proper_file_path\n",
    "    \n",
    "    else:\n",
    "        print(\"Not a dbfs filepath\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081dd9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c632fa4f",
   "metadata": {},
   "source": [
    "#### Functions used for reading in the PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f83e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def _read_pdf(pdf_name):\n",
    "    print(f\"Reading the pdf: {pdf_name}\")\n",
    "    list_of_pages = []\n",
    "    try:\n",
    "        reader = PdfReader(pdf_name)\n",
    "        for page in reader.pages:\n",
    "            list_of_pages.append(page.extract_text())\n",
    "            \n",
    "    except Exception as exc:\n",
    "        print(\"An error occurred. The exception is given below:\")\n",
    "        print(exc)\n",
    "        list_of_pages.append(fallback_text_extraction(pdf_name))\n",
    "        \n",
    "    except FileNotFoundError as fnfe:\n",
    "        print(f\"FILE NOT FOUND!\\n{fnfe}\")\n",
    "        \n",
    "    \n",
    "    return list_of_pages\n",
    "\n",
    "\n",
    "\n",
    "def _extract_useful_text(list_of_pages, useful_text, *reports_of_interest):\n",
    "    print(f\"Extracting the only the useful pages from the selected pdf file.\\nThe useful pages contain: {reports_of_interest}\")\n",
    "    useful_text_dict = {}\n",
    "    for report_id in reports_of_interest:\n",
    "        useful_text_dict[report_id] = []\n",
    "        for text in list_of_pages:\n",
    "            if report_id in text:\n",
    "                useful_text_dict[report_id].append(text)\n",
    "        \n",
    "    for report_id in useful_text_dict.keys():\n",
    "        useful_text.append(\"\\n\".join(useful_text_dict[report_id]))\n",
    "    \n",
    "    num_required_reports = len(reports_of_interest)\n",
    "    num_extracted_pages = len(useful_text)\n",
    "    print(f\"{num_extracted_pages} pages have been extracted: \")\n",
    "    \n",
    "    if num_extracted_pages > num_required_reports:\n",
    "        warnings.warn(\"\\n\\nIncomplete extraction! 1 or more extra reporting ids extracted from pdf\\n\")\n",
    "        print(f\"{num_extracted_pages} pages were extracted instead of {num_required_reports}\\n\")\n",
    "        \n",
    "    if num_extracted_pages < num_required_reports:\n",
    "        warnings.warn(\"\\n\\nIncomplete extraction! 1 or more reporting ids missing from pdf\\n\")\n",
    "        print(f\"{num_extracted_pages} pages were extracted instead of {num_required_reports}\\n\")\n",
    "                \n",
    "            \n",
    "\n",
    "            \n",
    "@function_name_decorator\n",
    "def _validate_extracted_text(useful_text, *reports_of_interest):\n",
    "    print(f\"Checking if all the extracted pages required were gotten. That is: {reports_of_interest}\")\n",
    "    check_list = list(reports_of_interest).copy()\n",
    "    \n",
    "    for report_id in reports_of_interest:\n",
    "        for text in useful_text:\n",
    "            if report_id in text:\n",
    "                check_list.remove(report_id)\n",
    "                   \n",
    "    if len(check_list) == 0:\n",
    "        print(\"All required report_ids successfully extracted\")\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        warnings.warn(f\"\\n\\nThe following report_id(s) was/were not found\\n: {check_list}\\n\")\n",
    "        return False\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a183c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74d6fdd7",
   "metadata": {},
   "source": [
    "#### Functions for creating the dataframes used for the rest of the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dae09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def _column_list_maker(number_of_columns):\n",
    "    print(\"This function creates a list of column names such as 'col1', 'col2', and so on from the the number of columns\")\n",
    "    list_of_columns = []\n",
    "    \n",
    "    for col_number in range(number_of_columns):\n",
    "        list_of_columns.append(f\"col{col_number}\")\n",
    "    \n",
    "    return list_of_columns\n",
    "\n",
    "\n",
    "@function_name_decorator\n",
    "def _optimal_df_col_names(rows):\n",
    "    print(\"This function determines the uptimal number of columns required to generate the dataframe.\")\n",
    "    \n",
    "    max_col_number = 7\n",
    "    optimal_col_names = []\n",
    "    while max_col_number < np.inf:\n",
    "        column_names = _column_list_maker(max_col_number)\n",
    "        \n",
    "        try:\n",
    "            df = pd.DataFrame(rows, columns = column_names)\n",
    "            \n",
    "            optimal_col_names = column_names.copy()\n",
    "            \n",
    "            print(f\"{max_col_number} columns is the optimal number of columns! Creating the DataFrame...\")\n",
    "            \n",
    "            break\n",
    "            \n",
    "        except (AssertionError, ValueError) as e:\n",
    "            print(f\"{max_col_number} columns is not the optimal number of columns! Trying {max_col_number + 1} columns next!\")\n",
    "            max_col_number += 1\n",
    "        \n",
    "        if max_col_number > 20:\n",
    "            print(f\"There are no reasonable number of columns!\")\n",
    "            break\n",
    "            \n",
    "        \n",
    "    \n",
    "    return optimal_col_names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@function_name_decorator\n",
    "def _dframe_maker(text_list, *reports_of_interest):\n",
    "    print(f\"This function creates the Dataframes for each report of interest: {reports_of_interest}\")\n",
    "    df_dict = {}\n",
    "    for report_id in reports_of_interest:\n",
    "        df_dict[report_id] = \"\"\n",
    "    for text in text_list:\n",
    "        # Create a DataFrame using the provided column names\n",
    "        #column_names = ['col0', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col7', 'col8', 'col9', 'col10']\n",
    "\n",
    "        # Preprocess the data to create a list of lists representing rows\n",
    "        rows = []\n",
    "        for line in text.split('\\n'):\n",
    "            if line.strip():\n",
    "                row = line.split()\n",
    "                rows.append(row)\n",
    "        print(f\"The number of rows is {len(rows)}\")\n",
    "    \n",
    "        # Convert the list of lists to a DataFrame\n",
    "        column_names = _optimal_df_col_names(rows)\n",
    "        df = pd.DataFrame(rows, columns=column_names)\n",
    "        \n",
    "        for report_id in reports_of_interest:\n",
    "            if report_id in text:\n",
    "                print(f\"report_id = {report_id}\")\n",
    "                df_dict[report_id] = df\n",
    "                \n",
    "        \n",
    "        #print(df)\n",
    "        \n",
    "    return df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f48b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a66b7b8f",
   "metadata": {},
   "source": [
    "#### Function for extracting the current month from the folderpath from the widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "months_list = [\"january\", \"february\", \"march\", \"april\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "\n",
    "#folder_path = dbutils.widgets.get(\"FolderPath\")\n",
    "\n",
    "@function_name_decorator\n",
    "def extract_month_from_folderpath(folder_path, months_list):\n",
    "    print(\"This function is used to get the month value from the folder path otherwise, it uses the current month\")\n",
    "    current_month = \"\"\n",
    "    if os.path.basename(folder_path).lower() in months_list:\n",
    "        current_month = os.path.basename(folder_path)\n",
    "\n",
    "    else:\n",
    "        warnings.warn(\"Month value not on file name(s). Using current month as Default\")\n",
    "        today = datetime.today()\n",
    "        month_num = today.month\n",
    "        current_month = calendar.month_name[month_num]\n",
    "\n",
    "    return current_month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88fa8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cf7a025",
   "metadata": {},
   "source": [
    "#### Function for getting the folderpaths for each day within the main FolderPath provided by the widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def get_folder_paths(folder_path):\n",
    "    print(\"This function pools all the folder paths for all the days within the month folder\")\n",
    "    try:\n",
    "        day_folder_paths = [folder.path for folder in dbutils.fs.ls(folder_path)]\n",
    "\n",
    "    except  (dbutils.dbutils.FileNotFoundError, dbutils.Exception.AttributeError) as e:\n",
    "        print(f\"File not found because the folder is empty or another error occurred:\\nPython error: {e}\")\n",
    "        dbutils.notebook.exit(f\"File not found: {e}\")\n",
    "        day_folder_paths = []\n",
    "\n",
    "    return day_folder_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30425290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "386487b9",
   "metadata": {},
   "source": [
    "#### Function for getting the exact file paths within the folderpaths for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def get_report_pdf_filepaths(folder_path, day_folder_paths, current_month):\n",
    "    print(\"This function is used to get all the report pdf files that are required for a particular day.\")\n",
    "    file_dict = {}\n",
    "    days_list = []\n",
    "    report_pdfs = []\n",
    "    \n",
    "    for num in range(len(day_folder_paths)):\n",
    "        days_list.append(day_folder_paths[num].partition(current_month)[-1].split(\"/\")[1])\n",
    "\n",
    "    for day in days_list:\n",
    "        file_dict[day] = folder_path + \"/\" + day\n",
    "\n",
    "    report_folders = {}\n",
    "    for key in file_dict.keys():\n",
    "        for file in dbutils.fs.ls(file_dict[key]):\n",
    "            #if \"chipper\" not in file.name.lower() or \"business\" not in file.name.lower() or \"paga\" not in file.name.lower() or \"virtual\" not in file.name.lower():\n",
    "                \n",
    "            report_folders[key] = file.path\n",
    "\n",
    "    for path in report_folders.values():\n",
    "        path_list = []\n",
    "        path_list.extend(dbutils.fs.ls(path))\n",
    "\n",
    "        path_list_copy = path_list.copy()\n",
    "\n",
    "        for path in path_list_copy:\n",
    "            report_pdfs.append(path.path)\n",
    "\n",
    "    pdf_file_paths = [pdf_file for pdf_file in report_pdfs if pdf_file.endswith('.pdf') or pdf_file.endswith('.pdf')]\n",
    "    pdf_file_paths.sort(key=lambda pdf_file: dbutils.fs.ls(pdf_file)[-1].modificationTime, reverse=True)\n",
    "\n",
    "    return pdf_file_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e152af5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68852d83",
   "metadata": {},
   "source": [
    "#### Function for deriving the dates of the files we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2894c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def get_desired_dates():\n",
    "    print(\"This function gets all the desired dates based on what day it is. Every other weekday picks the date before that day\")\n",
    "    print(\"But on Tuesday, dates for Saturday, Sunday, and Monday are selected\")\n",
    "    print(\"For Monday, Friday's date is selected\")\n",
    "    desired_dates = []\n",
    "    weekday = current_day.weekday()\n",
    "    \n",
    "    if weekday == 1:\n",
    "        for days_ago in range(1, 4):\n",
    "            desired_dates.append((current_day - timedelta(days=days_ago)).strftime('%d-%m-%Y')) #From Tuesday, pick Sat,Sun,Mon\n",
    "    else:\n",
    "        desired_dates.append((current_day - timedelta(days=1)).strftime('%d-%m-%Y'))  # Pick the day before\n",
    "        \n",
    "    print(f\"The files to be transformed have the following desired date(s):\")\n",
    "    for num, date in enumerate(desired_dates):\n",
    "        print(f\"File {num}: {date}\")\n",
    "        \n",
    "    return desired_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f64412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "582d71e9",
   "metadata": {},
   "source": [
    "#### Function for extracting the desired files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b978d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def get_desired_files(pdf_file_paths, desired_dates, desired_date_formats):\n",
    "    print(\"This function is for selecting the desired files based on the desired dates\")\n",
    "    \n",
    "    latest_file_paths = {}\n",
    "\n",
    "    for file_path in pdf_file_paths:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        #print(file_name)\n",
    "        #date_match = re.search(r'(\\d{1,2})[-_]?([A-Za-z]{3,})[-_]?(\\d{4})', file_name)\n",
    "        date_match = re.search(r'(20\\d{2})(0[1-9]|1[0-2])(0[1-9]|[1-2][0-9]|3[01])[-_]?', file_name)\n",
    "        if date_match:\n",
    "            day = date_match.group(3)\n",
    "            month = date_match.group(2)\n",
    "            year = date_match.group(1)\n",
    "            date_string = f\"{day}-{month}-{year}\"\n",
    "            print(date_string)\n",
    "        else:\n",
    "            print(f\"No date found in file name: {file_name}\")\n",
    "            continue\n",
    "        \n",
    "        latest_file_paths[date_string] = []\n",
    "        for date_format in desired_date_formats:\n",
    "            try:\n",
    "                file_date = datetime.strptime(date_string, date_format).strftime('%d-%m-%Y')\n",
    "                if file_date in desired_dates: #or file_date == current_day.strftime('%d-%m-%Y'):  # Also include files with the current date\n",
    "                    latest_file_paths[date_string].append(file_path)\n",
    "                    print(f\"File found for the desired date: {file_date}\")\n",
    "                    print(file_path)\n",
    "\n",
    "                else:\n",
    "                    latest_file_paths.pop(date_string)\n",
    "                    \n",
    "            except ValueError:\n",
    "                continue\n",
    "    print(\"The files we want are given below:\")\n",
    "    for date in latest_file_paths.keys():\n",
    "        print(f\"{date}: {latest_file_paths[date]}\")\n",
    "        \n",
    "    return latest_file_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1956120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0edd97de",
   "metadata": {},
   "source": [
    "#### Function for getting the country name from the filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca68ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def _get_country_name(file_path):\n",
    "    print(\"This function gets the country name from the file path\")\n",
    "    country_name = file_path.split(\"AFRICAN COUNTRIES/\")[1].split(\"/\")[0].strip()\n",
    "    return country_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e751b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08612c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def initialize_vss_dicts():\n",
    "    print(\"This function initializes the dictionaries for storing extracted target values. The dictionaries are for VSS-110, VSS-120, VSS-210\")\n",
    "    \n",
    "    vss110_dict = {\n",
    "    \n",
    "    \"NET_SETTLEMENT\":[],\n",
    "    \"TOTAL_REIMBURSEMENT\": [],\n",
    "    \"TOTAL_CHARGE\": [],\n",
    "    \"DATE\": []\n",
    "    \n",
    "    }\n",
    "\n",
    "    vss120_dict = {\n",
    "    \n",
    "    \"CLEARING_AMOUNT\": [],      \n",
    "    }\n",
    "\n",
    "    vss210_dict = {\n",
    "    \"OPTIONAL_FEE\": []\n",
    "    }\n",
    "\n",
    "    return vss110_dict, vss120_dict, vss210_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818e48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5489b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def _create_dataframes_from_pdf(pdf_files):\n",
    "    print(\"This function is used to create the dataframes for each of VSS-110, VSS-120, VSS-210\")\n",
    "    no_files = False\n",
    "\n",
    "    df_dict = {}\n",
    "    if pdf_files:\n",
    "        #temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "        useful_text = []\n",
    "        for file_path in pdf_files:\n",
    "            proper_file_path = _convert_dbfs_file_path(file_path)\n",
    "\n",
    "            list_of_pages = _read_pdf(proper_file_path)\n",
    "\n",
    "            _extract_useful_text(list_of_pages, useful_text, \"VSS-110\", \"VSS-120\", \"VSS-210\")\n",
    "\n",
    "            _validate_extracted_text(useful_text, \"VSS-110\", \"VSS-120\", \"VSS-210\")\n",
    "\n",
    "            df_dict = _dframe_maker(useful_text, \"VSS-110\", \"VSS-120\", \"VSS-210\")\n",
    "\n",
    "    else:\n",
    "        no_files = True\n",
    "        df_dict[\"VSS-110\"] = \"\"\n",
    "        df_dict[\"VSS-120\"] = \"\"\n",
    "        df_dict[\"VSS-210\"] = \"\"\n",
    "\n",
    "    return df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1665cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e370556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def proc_date(dframe, date_string):\n",
    "    \n",
    "    df = dframe.copy()\n",
    "    df_columns = list(df.columns)\n",
    "    mask_sum = []\n",
    "    col_labels = []\n",
    "    idx_list = []\n",
    "    for col in dframe.columns:\n",
    "        proc_mask = df[col].fillna(\"-\").str.lower().str.startswith(\"proc\")\n",
    "        date_mask = df[col].fillna(\"-\").str.lower().str.startswith(\"date\")\n",
    "        \n",
    "        try:\n",
    "            if sum(proc_mask) > 0:\n",
    "                idx_list.append(df[proc_mask].index.values[0])\n",
    "                mask_sum.append(sum(proc_mask))\n",
    "                col_labels.append(col)\n",
    "\n",
    "            if sum(date_mask) > 0:\n",
    "                mask_sum.append(sum(date_mask))\n",
    "                col_labels.append(col)\n",
    "                break\n",
    "\n",
    "        except (TypeError, IndexError) as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "    try:\n",
    "        col_label0 = df_columns.index(col_labels[0])\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        print(f\"Using the date value of the file name: {date_string}\")\n",
    "        return date_string\n",
    "    try:    \n",
    "        col_label1 = df_columns.index(col_labels[1])\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        print(f\"Using the date value of the file name: {date_string}\")\n",
    "        return date_string\n",
    "            \n",
    "    if col_label0 + 1 == col_label1:\n",
    "        #date_label_index = df_columns.index(col_index[1])\n",
    "        date_col = df_columns[col_label1 + 1]\n",
    "    else:\n",
    "        print(\"After the else\")\n",
    "        print(f\"Using the date value of the file name: {date_string}\")\n",
    "        return date_string\n",
    "    \n",
    "    return df.at[idx_list[0], date_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304fd52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a1aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def _convert_db_cr(number):\n",
    "    print(\"This function is used to convert Debits with 'db' attached to them to Negative numbers, and Credits to Positive numbers\")\n",
    "    if not isinstance (number, str):\n",
    "        return number\n",
    "\n",
    "    if \"db\" in number.lower():\n",
    "        new_num = \"-\" + number.lower().replace(\"db\", \"\")\n",
    "        return new_num\n",
    "        \n",
    "    elif \"cr\" in number.lower():\n",
    "        new_num = number.lower().replace(\"cr\", \"\")\n",
    "        return new_num\n",
    "        \n",
    "    else:\n",
    "        return number\n",
    "\n",
    "#proc_date(df)\n",
    "\n",
    "@function_name_decorator\n",
    "def _convert_string_numbers(num):\n",
    "    print(\"This function is used to convert string numbers into float numbers\")\n",
    "    num = str(num)\n",
    "    num = num.replace(\",\", \"\")\n",
    "    \n",
    "    try:\n",
    "        num = float(num)\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"The string value you are trying to convert doesn't contain any numbers\\nPython Error:{e.with_traceback()}\")\n",
    "    \n",
    "    else:\n",
    "        return num\n",
    "    \n",
    "    return num\n",
    "\n",
    "\n",
    "\n",
    "@function_name_decorator\n",
    "def _convert_number_strings(number):\n",
    "    print(\"This function is used to convert numbers back to strings\")\n",
    "    number_str = str(number)\n",
    "    \n",
    "    partitioned_num = number_str.partition(\".\")\n",
    "    print(partitioned_num)\n",
    "    if len(partitioned_num[0])<= 3:\n",
    "        if len(partitioned_num[-1]) == 2:\n",
    "            return number_str      \n",
    "        elif len(partitioned_num[-1]) == 1:\n",
    "            return number_str + \"0\"     \n",
    "        \n",
    "        elif len(partitioned_num[-1]) > 2:\n",
    "            result = str((lambda x: round(float(\"0.\" + x), 2))(partitioned_num[-1]))[-2:]\n",
    "            new_result = partitioned_num[0] + \".\"+result\n",
    "            #print(new_result)\n",
    "            if \".\" in new_result.partition(\".\")[-1]:\n",
    "                #print(\"this became true\")\n",
    "                new_result = new_result.partition(\".\")[0] + new_result.partition(\".\")[-1]\n",
    "                return new_result\n",
    "            \n",
    "            else: \n",
    "                return new_result\n",
    "        \n",
    "        else:\n",
    "            partitioned_num[0] + \".\" + \"00\"\n",
    "            \n",
    "    length = len(partitioned_num[0])\n",
    "\n",
    "    if length <= 3:\n",
    "        return number_str  # No need for commas\n",
    "\n",
    "    formatted = []\n",
    "    for i in range(length):\n",
    "        if i > 0 and (length - i) % 3 == 0:\n",
    "            formatted.append(',')\n",
    "        formatted.append(number_str[i])\n",
    "    \n",
    "    if len(partitioned_num[-1]) == 2:\n",
    "        result = ''.join(formatted) + \".\" + partitioned_num[-1]\n",
    "        \n",
    "    elif len(partitioned_num[-1]) == 1:\n",
    "        result = ''.join(formatted) + \".\" + partitioned_num[-1] + \"0\"\n",
    "        \n",
    "    elif len(partitioned_num[-1]) > 2:\n",
    "        result = ''.join(formatted) + \".\" + str((lambda x: round(float(\"0.\" + x), 2))(partitioned_num[-1]))[-2:]\n",
    "        \n",
    "    else:\n",
    "        result = ''.join(formatted) + \".\" + partitioned_num[-1] + \"00\"\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ef9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def net_settlement(df):\n",
    "    print(\"This function is used for extracting the Net Settlement from the dataframe generated.\")\n",
    "    net_filter = df[\"col0\"].str.lower() == \"net\"\n",
    "    settlement_filter = df[\"col1\"].str.lower() == \"settlement\"\n",
    "    amount_filter = df[\"col2\"].str.lower() == \"amount\"\n",
    "    \n",
    "    try:\n",
    "        result = df.loc[(net_filter) & (settlement_filter) & (amount_filter), \"col5\"]\n",
    "        raw_result = 0\n",
    "    \n",
    "        \n",
    "        raw_result = result.values[0]\n",
    "\n",
    "    except (IndexError, Exception) as e:\n",
    "        print(f\"Null values/empty space in total_reimbursement column.\\nAlso, it could be truncated and/or incomplete pdf file.\\nPython error:{e}\")\n",
    "    \n",
    "    net_settlement = _convert_db_cr(raw_result)\n",
    "    \n",
    "    return net_settlement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ba572e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b59ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def _reimbursement_calculator(df, reimbursement_type):\n",
    "    print(\"This function checks if the reimbursement value is the correct value via calculation\")\n",
    "    if reimbursement_type.lower() == \"credit\":\n",
    "        value_col = \"col3\"\n",
    "    elif reimbursement_type.lower() == \"debit\":\n",
    "        value_col = \"col4\"\n",
    "    \n",
    "    raw_result = 0\n",
    "    try:\n",
    "        total_filter = df[\"col0\"].str.lower() == \"total\"\n",
    "        reimbursement_filter = df[\"col1\"].str.lower() == \"reimbursement\"\n",
    "        fees_filter = df[\"col2\"].str.lower() == \"fees\"\n",
    "\n",
    "        result = df.loc[(total_filter) & (reimbursement_filter) &(fees_filter), value_col]\n",
    "    #print(result)\n",
    "    \n",
    "        raw_result = result.values[0]\n",
    "    except IndexError as e:\n",
    "        print(f\"Please ensure you use VSS-110 pdf file if not you get index error. \\nAlso, it could be truncated and/or incomplete pdf file\")\n",
    "        print(e)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"There is most likely no information in the dataframe.\\nPython error:{e}\")\n",
    "    \n",
    "    raw_result = _convert_db_cr(raw_result)\n",
    "    raw_result = _convert_string_numbers(raw_result)\n",
    "    \n",
    "    try:\n",
    "        raw_result = float(raw_result)\n",
    "\n",
    "    except (ValueError, Exception) as e:\n",
    "        print(e)\n",
    "        return raw_result\n",
    "\n",
    "    return raw_result\n",
    "    \n",
    "\n",
    "#reimbursement_calculator(df, \"credit\")\n",
    "\n",
    "\n",
    "@function_name_decorator\n",
    "def _charges_calculator(df, charges_type):\n",
    "    print(\"This function calculates the charges\")\n",
    "    if charges_type.lower() == \"credit\":\n",
    "        value_col = \"col3\"\n",
    "    elif charges_type.lower() == \"debit\":\n",
    "        value_col = \"col4\"\n",
    "    \n",
    "    try:\n",
    "        total_filter = df[\"col0\"].str.lower() == \"total\"\n",
    "        \n",
    "        visa_filter = df[\"col1\"].str.lower() == \"visa\"\n",
    "        \n",
    "        charges_filter = df[\"col2\"].str.lower() == \"charges\"\n",
    "        \n",
    "        result = df.loc[(total_filter) & (visa_filter) &(charges_filter), value_col]\n",
    "\n",
    "    except (IndexError, Exception) as e:\n",
    "        print(f\"Total and/or visa and/or charges missing from pdf file.\\nPdf file might be truncated/incomplete.\\nPython error: {e}\")\n",
    "    #print(result)\n",
    "    raw_result = 0\n",
    "\n",
    "    try:\n",
    "        raw_result = result.values[0]\n",
    "    except IndexError as e:\n",
    "        print(f\"Please ensure you use VSS-110 pdf file if not you get index error. \\nAlso, it could be truncated and/or incomplete pdf file\")\n",
    "        print(e)\n",
    "    \n",
    "    raw_result = _convert_db_cr(raw_result)\n",
    "    raw_result = _convert_string_numbers(raw_result)\n",
    "    \n",
    "    try:\n",
    "        raw_result = float(raw_result)\n",
    "\n",
    "    except (ValueError, Exception) as e:\n",
    "        print(e)\n",
    "        return raw_result\n",
    "\n",
    "    return raw_result\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "@function_name_decorator    \n",
    "def _totals(df):\n",
    "    print(\"This function adds up the total reimbursement and charges\")\n",
    "    #print(f\"adding up {total_visa_charge_credit(df)} and {total_reimbursement_credit(df)}\")\n",
    "    total_reimbursement = _reimbursement_calculator(df, \"credit\") +  _charges_calculator(df, \"credit\")\n",
    "    total_charge = _reimbursement_calculator(df, \"debit\") + _charges_calculator(df, \"debit\")\n",
    "    \n",
    "    return total_reimbursement, total_charge\n",
    "    \n",
    "\n",
    "\n",
    "#totals(df_dict[\"VSS-110\"])\n",
    "#vss110_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23db89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def _total_issuer_interchange(df):\n",
    "    print(\"This function helps to extract the total issuer interchange.\")\n",
    "    try:\n",
    "    \n",
    "        total_filter = df[\"col0\"].str.lower() == \"total\"\n",
    "        \n",
    "        issuer_filter = df[\"col1\"].str.lower() == \"issuer\"\n",
    "        \n",
    "        interchange_filter = df[\"col2\"].str.lower() == \"interchange\"\n",
    "        \n",
    "        result = df.loc[(total_filter) & (issuer_filter) & (interchange_filter), \"col4\"]\n",
    "\n",
    "    except (IndexError, Exception) as e:\n",
    "        print(f\"Total and/or issuer and/or interchange missing from pdf file.\\nPdf file might be truncated/incomplete\\n.Python error:{e}\")\n",
    "    #print(result)\n",
    "\n",
    "    except (KeyError) as e:\n",
    "        print(f\"There is most likely no file. Python error is: {e}\")\n",
    "    \n",
    "    try:\n",
    "        if result.isnull().values[0]:\n",
    "            warnings.warn(\"Null values/empty space in total_reimbursement column.\")\n",
    "            return 0\n",
    "    except IndexError as e:\n",
    "        print(f\"Please ensure you use VSS-120 pdf file if not you get index error\")\n",
    "        print(e)\n",
    "        return 0\n",
    "\n",
    "    raw_result = result.values[0]\n",
    "    \n",
    "    total_issuer_interchange_val = _convert_db_cr(raw_result)\n",
    "    #raw_result = convert_string_numbers(raw_result)\n",
    "\n",
    "\n",
    "    #total_issuer_interchange_val = convert_db_cr(raw_result)\n",
    "    \n",
    "    \n",
    "    return total_issuer_interchange_val\n",
    "\n",
    "#total_issuer_interchange(df_dict[\"VSS-120\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c83e880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96fc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _total_currency_conversion(df):\n",
    "    print(\"This function helps to calculate the total currency conversion\")\n",
    "    try:\n",
    "        total_filter = df[\"col0\"].str.lower() == \"total\"\n",
    "        currency_filter = df[\"col1\"].str.lower() == \"currency\"\n",
    "        conversion_filter = df[\"col2\"].str.lower() == \"conversion\"\n",
    "        fees_filter = df[\"col3\"].str.lower() == \"fees\"\n",
    "\n",
    "    except (KeyError, Exception) as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        title_row = df.loc[(total_filter) & (currency_filter) & (conversion_filter), :].index[0]\n",
    "        value_row = title_row + 1\n",
    "\n",
    "    except (IndexError, Exception) as e:\n",
    "        print(f\"Total and/or currency and/or conversion are not present in the pdf file\\nPdf File might be truncated/incomplete.\\nPython error:{e}\")\n",
    "    \n",
    "    try:\n",
    "        result = df.iloc[value_row, :].dropna().to_list()[-1]\n",
    "    \n",
    "    except (IndexError, Exception) as e:\n",
    "        print(e)\n",
    "        result = 0\n",
    "    \n",
    "    total_currency_conversion = _convert_db_cr(result)\n",
    "    \n",
    "    return total_currency_conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb27bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c0bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def _select_parameters_vss110(df_dict, vss110_dict, date_string):\n",
    "    print(\"This function uses the other functions extract Total reimbursement, Total charge, Net settlement, and Date\")\n",
    "    df = df_dict[\"VSS-110\"]\n",
    "    #display(df)\n",
    "    if isinstance(df, str):\n",
    "        return vss110_dict\n",
    "    \n",
    "    \n",
    "    total_reimbursement, total_charge = _totals(df)\n",
    "    total_reimbursement = round(total_reimbursement, 2) \n",
    "    total_reimbursement = _convert_number_strings(total_reimbursement)\n",
    "    \n",
    "    total_charge = round(total_charge, 2)\n",
    "    total_charge = _convert_number_strings(total_charge)\n",
    "    \n",
    "    vss110_dict[\"TOTAL_REIMBURSEMENT\"].append(total_reimbursement)\n",
    "    vss110_dict[\"TOTAL_CHARGE\"].append(total_charge)\n",
    "    \n",
    "    \n",
    "    vss110_dict[\"NET_SETTLEMENT\"].append(net_settlement(df))\n",
    "    \n",
    "    vss110_dict[\"DATE\"].append(proc_date(df, date_string))\n",
    "    \n",
    "    \n",
    "    return vss110_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d792f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff959423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_parameters_vss120(df_dict, vss120_dict):\n",
    "    print(\"This function is used to extract the Clearing Amount\")\n",
    "    df = df_dict[\"VSS-120\"]\n",
    "    #display(df)\n",
    "    if isinstance(df, str):\n",
    "        return vss120_dict\n",
    "    \n",
    "    total_issuer_interchange_val = _total_issuer_interchange(df)\n",
    "    \n",
    "    vss120_dict[\"CLEARING_AMOUNT\"].append(_total_issuer_interchange(df))\n",
    "\n",
    "    return vss120_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070957c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b605359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_parameters_vss210(df_dict, vss210_dict):\n",
    "    df = df_dict[\"VSS-210\"]\n",
    "    #display(df)\n",
    "    if isinstance(df, str):\n",
    "        return vss210_dict\n",
    "    \n",
    "    total_currency_conversion_val = _total_currency_conversion(df)\n",
    "    vss210_dict[\"OPTIONAL_FEE\"].append(_total_currency_conversion(df))\n",
    "\n",
    "    return vss210_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b9128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff66dfe2",
   "metadata": {},
   "source": [
    "#### Function for doing extra column additions like Date and Date Modified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_name_decorator\n",
    "def _perform_final_df_configuration(dframe, date_string):\n",
    "    print(\"This function performs additional data cleaning and adds Transaction type and Date modified to the dataframe\")\n",
    "    \n",
    "    final_df = dframe.copy()\n",
    "    country_name = _get_country_name(folder_path)\n",
    "    print(country_name)\n",
    "    final_df[\"TRANSACTION_TYPE\"] = [country_name] * len(final_df)\n",
    "\n",
    "    #CurrentDate = dbutils.widgets.get(\"CurrentDate\")\n",
    "    CurrentDate = datetime.now()\n",
    "    final_df['Date_Modified'] = [CurrentDate] * len(final_df)\n",
    "\n",
    "    final_df = final_df[['NET_SETTLEMENT', 'TOTAL_REIMBURSEMENT', 'TOTAL_CHARGE','CLEARING_AMOUNT', 'OPTIONAL_FEE', 'TRANSACTION_TYPE', 'DATE', 'Date_Modified']]\n",
    "\n",
    "    final_df[\"DATE\"] = pd.to_datetime(final_df[\"DATE\"])\n",
    "    final_df['Date_Modified'] = pd.to_datetime(final_df['Date_Modified'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "    final_df.loc[:, ['NET_SETTLEMENT', 'TOTAL_REIMBURSEMENT', 'TOTAL_CHARGE','CLEARING_AMOUNT', 'OPTIONAL_FEE']] = final_df[['NET_SETTLEMENT', 'TOTAL_REIMBURSEMENT', 'TOTAL_CHARGE','CLEARING_AMOUNT', 'OPTIONAL_FEE']].applymap(_convert_string_numbers)\n",
    "\n",
    "    print(f\"The final structure of the dataframe for the following date {date_string} is given below:\")\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db206cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25856844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec9f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_transformations(latest_file_paths, desired_dates):\n",
    "    if len(latest_file_paths) == 0:\n",
    "        warnings.warn(f\"File(s) not found for desired dates: {desired_dates}.\\nNotebook closed\")\n",
    "        dbutils.notebook.exit(f\"File(s) not found for desired dates: {desired_dates}.\\nNotebook closed\")\n",
    "\n",
    "    else:\n",
    "        print(\"Performing the transformations for the files that came on the following days:\")\n",
    "        for date_string in latest_file_paths.keys():\n",
    "            print(date_string)\n",
    "            \n",
    "        combined_dataframes_dict = {}\n",
    "        for date_string in latest_file_paths.keys():\n",
    "            print(f\"File transformation for file on date: {date_string}\")\n",
    "            combined_dataframes_dict[date_string] = []\n",
    "            pdf_files = latest_file_paths[date_string]\n",
    "            \n",
    "            df_dict = _create_dataframes_from_pdf(pdf_files)\n",
    "            vss110_dict, vss120_dict, vss210_dict = initialize_vss_dicts()\n",
    "            print(vss120_dict)\n",
    "            vss110_dict = _select_parameters_vss110(df_dict, vss110_dict, date_string)\n",
    "\n",
    "            vss120_dict = _select_parameters_vss120(df_dict, vss120_dict)\n",
    "\n",
    "            vss210_dict = _select_parameters_vss210(df_dict, vss210_dict)\n",
    "            \n",
    "            vss110_df = pd.DataFrame(vss110_dict)\n",
    "            vss120_df = pd.DataFrame(vss120_dict)\n",
    "            print(vss120_dict)\n",
    "            vss210_df = pd.DataFrame(vss210_dict)\n",
    "            \n",
    "            final_df = pd.concat([vss110_df, vss120_df, vss210_df], axis = 1)\n",
    "            print(final_df)\n",
    "            \n",
    "            final_df = _perform_final_df_configuration(final_df, date_string)\n",
    "            \n",
    "            combined_dataframes_dict[date_string].append(final_df)\n",
    "            print(\"\\n\\n\\n\")\n",
    "        return combined_dataframes_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c905dc59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _define_schema():\n",
    "    schema = StructType([\n",
    "        StructField(\"NET_SETTLEMENT\", FloatType(), True),\n",
    "        StructField(\"TOTAL_REIMBURSEMENT\", FloatType(), True),\n",
    "        StructField(\"TOTAL_CHARGE\", FloatType(), True),\n",
    "        StructField(\"CLEARING_AMOUNT\", FloatType(), True),\n",
    "        StructField(\"OPTIONAL_FEE\", FloatType(), True),\n",
    "        StructField(\"TRANSACTION_TYPE\", StringType(), True),\n",
    "        StructField(\"DATE\", DateType(), True),\n",
    "        StructField(\"Date_Modified\", TimestampType(), True)  # <-- corrected line\n",
    "    ])\n",
    "    \n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36bb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_spark_dataframe(combined_final_df):\n",
    "    schema = _define_schema()\n",
    "    database_table_dict = {}\n",
    "    for date in combined_final_df.keys():\n",
    "        print(f\"\\nThe final Dataframe for date: {date} is:\")\n",
    "        final_df = combined_final_df[date][0]\n",
    "        final_result = spark.createDataFrame(final_df, schema=schema)\n",
    "        database_table_dict[date] = final_result\n",
    "        display(final_result)\n",
    "    return final_result, database_table_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579eae02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97152ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _define_database_parameters():\n",
    "    jdbcHostname = dbutils.secrets.get(scope = \"azurekv-rpa1scope\", key = \"Hostname\")\n",
    "    jdbcPort = dbutils.secrets.get(scope = \"azurekv-rpa1scope\", key = \"port\")\n",
    "    jdbcDatabase = dbutils.secrets.get(scope = \"azurekv-rpa1scope\", key = \"database\")\n",
    "    jdbcUsername = dbutils.secrets.get(scope = \"azurekv-rpa1scope\", key = \"username\")\n",
    "    jdbcPassword = dbutils.secrets.get(scope = \"azurekv-rpa1scope\", key = \"jdbcpwd\")\n",
    "    table =\"Prepaid_Affliate_Report\"\n",
    "    jdbcDriver = dbutils.secrets.get(scope = \"azurekv-rpa1scope\", key = \"jdbcdriver\")\n",
    "    jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};database = {jdbcDatabase}\"\n",
    "    \n",
    "    return jdbcHostname, jdbcPort, jdbcDatabase, jdbcUsername, jdbcPassword, table, jdbcDriver, jdbcUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36725ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3227c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985a76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c2c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc832093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
